{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Extraction in Newsfeeds\n",
    "* Autor: Prof. Dr. Johannes Maucher\n",
    "* Datum: 17.11.2015\n",
    "\n",
    "[Übersicht Ipython Notebooks im Data Mining Praktikum](Data Mining Praktikum.html)\n",
    "\n",
    "#Einführung\n",
    "## Lernziele:\n",
    "\n",
    "In diesem Versuch sollen Kenntnisse in folgenden Themen vermittelt werden:\n",
    "\n",
    "* __RSS Feeds:__ Struktur von RSS Feeds analysieren und parsen mit dem _Universal Feed Parser_. \n",
    "* __Dokument Analyse:__ Die Häufigkeit aller Worte in einem Dokument (Inhalt des RSS Feeds) zählen und in einem Array verwalten. \n",
    "* __Merkmalsextraktion:__ Bestimmung von Merkmalen (hier auch: __Topics__) (Allgemein spricht man von Merkmalen. Im Fall, dass die NNMF auf Dokumente angewandt wird, werden die Merkmale auch mit __Topics__ oder __Themen__ bezeichnet) mit der \\emph{Non Negative Matrix Factorization}.\n",
    "* __Zuordnung__: Wie setzen sich die Topics aus den Wörtern zusammen? Wie stark sind die gefundenen Topics in den Artikeln vertreten?\n",
    "* __Dokument Clustering:__ Mit der NNMF kann auch ein Clustering realisiert werden. Jeder Topic repräsentiert ein Cluster. Jedes Dokument wird dem Cluster zugeordnet, dessen Topic am stärksten in ihm vertreten ist. \n",
    "\n",
    "Sämtliche Verfahren und Algorithmen werden in Python implementiert.\n",
    "\n",
    "## Theorie zur Vorbereitung\n",
    "\n",
    "Stellen Sie sich vor Sie möchten in eine eigene Webseite die RSS Feeds einer Menge von Nachrichtenservern einbinden. Da die unterschiedlichen Server wahrscheinlich Artikel zu den gleichen Themen anbieten, werden die Inhalte einiger Artikel ähnlich sein. Mit der __Nicht Negativen Matrixfaktorisierung (NNMF)__ kann für eine große Menge von Dokumenten eine Menge von Themen (Topics) ermittelt werden, auf die sich die Dokumente beziehen. Damit ist es u.a. möglich\n",
    "* die Dokumente thematisch zu ordnen\n",
    "* zu jedem Thema nur ein Dokument anzuzeigen\n",
    "\n",
    "### Ähnlichkeiten bestimmen und relevante Merkmale extrahieren\n",
    "\n",
    "Eine Sammlung von Dokumenten - in diesem Versuch die Menge aller Nachrichten der angegebenen Feeds - kann in einer Artikel/Wort-Matrix repräsentiert werden. Jede Zeile dieser Matrix gehört zu einem Dokument. Für jedes Wort, das mindestens in einem der Dokumente vorkommt, ist eine Spalte vorgesehen. Das Matrixelement in Zeile $i$, Spalte $j$ beschreibt wie häufig das Wort in Spalte $j$ im zur Zeile $i$ gehörenden Dokument vorkommt.\n",
    "\n",
    "Unter der Annahme, dass Artikel umso ähnlicher sind, je mehr Worte in diesen gemeinsam vorkommen, kann auf der Grundlage dieser Matrix die Ähnlichkeit zwischen den Artikeln berechnet werden. Hierzu könnte die Matrix z.B. einfach einem _Hierarchischen Clustering_ übergeben werden. Das hierarchische Clustering weist jedoch im Fall einer großen Menge von zu vergleichenden Objekten zwei wesentliche Nachteile auf: Erstens ist die wiederholte Berechnung der Distanzen zwischen allen Artikeln/Clustern extrem rechenaufwendig, zweitens ist die Darstellung einer großen Anzahl von Objekten im Dendrogramm nicht mehr übersichtlich. \n",
    "\n",
    "Für das Auffinden von Assoziationen zwischen Dokumenten hat sich in den letzten Jahren die Methode der __Nicht-Negativen Matrix Faktorisierung (NNMF)__ etabliert. Mit dieser Methode kann eine Menge von wesentlichen Merkmalen berechnet werden, anhand derer sich die Dokumente clustern lassen, d.h. Dokumente des gleichen Clusters repräsentieren das gleiche Merkmal (Thema). Ein solches Merkmal wird durch eine Menge von Worten beschrieben, z.B. $\\{$ _Paris, terror, IS_ $\\}$  oder $\\{$_refugee, syria, border_ $\\}$. Neben der Merkmalsextraktion stellt die relativ geringe Komplexität einen weiteren Vorteil der NNMF dar. Durch die Darstellung der Artikel/Wort-Matrix als Produkt von 2 Faktormatrizen müssen deutlich weniger Einträge gespeichert werden.\n",
    "\n",
    "### Nicht Negative Matrixfaktorisierung: Die Idee\n",
    "\n",
    "Die Artikel/Wort-Matrix wird im Folgenden mit $A$ bezeichnet. Sie besitzt $r$ Zeilen und $c$ Spalten, wobei $r$ die Anzahl der Artikel und $c$ die Anzahl der relevanten Worte in der Menge aller Artikel ist. Durch Multiplikation der Matrix $A$ mit dem Vektor $v$ (_wordvec_: Vektor der alle relevanten Worte enthält) werden die Worte den Artikeln $a$ (_articletitles_: Vektor der alle Artikeltitel enthält) zugeordnet:\n",
    "\n",
    "$$\n",
    "a=A*v.\n",
    "$$\n",
    "\n",
    "Die Idee der NNMF besteht darin die Matrix $A$ als Produkt zweier Matrizen $W$ und $H$ darzustellen,\n",
    "\n",
    "$$\n",
    "A=W*H\n",
    "$$\n",
    "\n",
    "wobei alle Elemente in $W$ und $H$ größer oder gleich Null sein müssen. Die Matrixmultiplikation erfordert, dass die Anzahl der Zeilen $m$ in $H$ gleich der Anzahl der Spalten in $W$ sein muss. \n",
    "Durch die Faktorisierung der Matrix $A$ wird die Zuordnung der Wörter des Wortvektors $v$  zu den Artikeln des Vektors $a$ in zwei Stufen zerlegt. \n",
    "\n",
    "$$\n",
    "f = H*v\n",
    "$$\n",
    "$$\n",
    "a = W*f \n",
    "$$\n",
    "\n",
    "In der ersten Stufe werden durch die Multiplikation von $v$ mit der Matrix $H$ die Wörter einem sogenannten Merkmalsvektor $f$ mit $m$ Elementen zugewiesen. In der zweiten Stufe werden durch die Multiplikation des Merkmalsvektor $f$ mit der Matrix $W$ die einzelnen Merkmale den Artikeln in $a$ zugeordnet. Die Matrix $H$ definiert also aus welchen Wörtern die Merkmale gebildet werden. Sie wird deshalb __Merkmalsmatrix__ genannt. Die Matrix $W$ hingegen beschreibt mit welchem Gewicht die einzelnen Merkmale in den verschiedenen Artikeln auftreten. Sie wird deshalb __Gewichtungsmatrix__ genannt.\n",
    "\n",
    "Daraus folgt: Wenn eine Faktorisierung der Matrix $A$ gefunden wird, dann werden damit auch relevante Merkmale, also die Themen, definiert, hinsichtlich derer die Artikel effizient kategorisiert werden. Durch die Matrixfaktorisierung wird eine __Merkmalsextraktion__ realisiert. \n",
    "\n",
    "### Berechnung der Matrixfaktoren\n",
    "\n",
    "Für die Berechnung der Faktoren wurde in [Lee, Algortihms for Non-negative Matrix Factorisation](http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf) eine iterative Methode vorgestellt, die derzeit wohl am häufigsten angewandt wird und auch in dieser Übung implementiert werden soll. Der Algorithmus besteht aus folgenden Schritten:\n",
    "* Gebe die zu faktorisierende Matrix $A$ ein. $r$ sei die Anzahl der Zeilen und $c$ die Anzahl der Spalten von $A$.\n",
    "* Wähle die Anzahl $m$ der Merkmale, mit $m<c$. _Tipp:_ Für $m$ sollte zunächst ein Wert im Bereich $15$ bis $30$ gewählt werden.\n",
    "* Lege eine $m \\times c$ Matrix $H$ an mit initial zufälligen Elementen (Anwendung der numpy Funktion _random.random()_)\n",
    "* Lege eine $r \\times m$ Matrix $W$ an mit initial zufälligen Elementen (Anwendung der numpy Funktion _random.random()_)\n",
    "* Wiederhole bis maximale Anzahl der Iteration erreicht oder Kosten $k$ unter vordefinierter Schwelle:\n",
    "\n",
    "\t* Berechne aktuelles Produkt $B=W*H$ und bereche die Kostenfunktion \n",
    "\t\t$$\n",
    "\t\t\tk=\\left\\| A - B \\right\\|^2 = \\sum\\limits_{i,j} \\left(A_{i,j} - B_{i,j}\\right)^2\n",
    "\t\t$$ \n",
    "\t* Anpassung der Matrix $H$ durch folgende Neuberechnung der Matrixelemente\n",
    "    \n",
    "\t\t$$\n",
    "\t\tH_{i,j} := H_{i,j} \\frac{(W^T*A)_{i,j}}{(W^T*W*H)_{i,j}}\n",
    "\t\t$$\n",
    "        \n",
    "\t* __Nach__ der Anpassung der Matrix $H$: Anpassung der Matrix $W$ durch folgende Neuberechnung der Matrixelemente\n",
    "    \n",
    "\t\t$$\n",
    "\t\tW_{l,i} := W_{l,i} \\frac{(A*H^T)_{l,i}}{(W*H*H^T)_{l,i}}\n",
    "\t\t$$\n",
    "\n",
    "In [Lee, Algortihms for Non-negative Matrix Factorisation](http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf) ist bewiesen, dass durch die o.g. Anpassungsroutinen die Kosten $k$ monoton abnehmen und in einem Minimum konvergieren. Der Algorithmus ist jedoch nicht optimal weil das gefundene Minimum ein lokales Minimum sein kann."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Vor dem Versuch zu klärende Fragen\n",
    " \n",
    " * Was versteht man unter Artikel/Wort-Matrix? Wie wird diese im aktuellen Versuch gebildet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Unter einer Artikel/Wort-Matrix versteht man einen zweidimensionalen Array, bei dem jede Zeile ein Dokument(Artikel) bezeichnet. Jede Spalte beschreibt dabei wie oft ein Wort bzw. eine Phrase in einem Artikel vorkommt.**  \n",
    "**Diese sieht dann zum Beispiel so aus:**  \n",
    "```\n",
    "                                        Word1   W2   W3   W4   W5   WX\n",
    "                                            +    +    +    +    +    +\n",
    "                                            |    |    |    |    |    |\n",
    "                                            v    v    v    v    v    v\n",
    "                                          +----------------------------+\n",
    "                        Article1  +--->   |  1 |  0 |  4 |  9 |  0 |  1\n",
    "                                          +----------------------------+\n",
    "                        Article2  +--->   |  0 |  2 |  4 |  2 |  1 |  0\n",
    "                                          +----------------------------+\n",
    "                        Article3  +--->   |  1 |  0 |  5 |  6 |  2 |  3\n",
    "                                          +---------------------+------+\n",
    "                        ArticleX  +--->   |  x |  y |  z |  v |  w |  k\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Wie multipliziert man die Matrix\n",
    "    $$\n",
    "    A= \\left( \\begin{array}{cccc}\n",
    "a_{00} & a_{01} & a_{02} & a_{03} \\\\ \n",
    "a_{10} & a_{11} & a_{12} & a_{13} \\\\ \n",
    "a_{20} & a_{21} & a_{22} & a_{23}\n",
    "\\end{array} \\right)\n",
    "    $$\n",
    "    mit dem Vektor  \n",
    "    $$\n",
    "    v=\\left( \\begin{array}{c}\n",
    "v_{0} \\\\ \n",
    "v_{1} \\\\ \n",
    "v_{2} \\\\ \n",
    "v_{3}\n",
    "\\end{array} \\right)\n",
    "    $$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Bei der Matrix-Vektor Multiplikation wird immer ein Wert einer Zeile mit einem Wert einer Spalte multipliziert und dieser Wert dann mit dem Rest der Werte aus der selben Zeile aufsummiert.**  \n",
    "**Das Ergebnis hierbei ist immer ein Vektor der selben Dimension wie die des Ausgangsvektors.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![matrixVektorMulti](./matrixvectormultiplication.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Was versteht man im Kontext der NNMF unter\n",
    "    * Merkmalsmatrix\n",
    "    * Gewichtsmatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Merkmalsmatrix:**  \n",
    "Durch die Multiplikation des Merkmalsvektor, welcher wiederrum durch die Multiplikation der Matrix mit dem WordVec ensteht, mit der Matrix errechnen sich die einzelnen Merkmale die den Artikeln zugeordnet sind.\n",
    "\n",
    "Die Merkmalsmatrix definiert also aus welchen Wörtern die Merkmale gebildet werden.  \n",
    "\n",
    "**Gewichtsmatrix:**  \n",
    "Die Gewichtsmartix beschreibt mit welchem Gewicht die einzelnen Merkmale der Merkmalsmatrix in den verschiedenen Artikeln auftreten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Wie definieren die Zeilen der Merkmalsmatrix die einzelnen Merkmale (Topics)?\n",
    "* Was definieren die Zeilen der Gewichtungsmatrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Merkmalsmatrix:**  \n",
    "Jede Zeile beschreibt ein Merkmal, jede Spalte ein Wort. Je höher der Wert pro Spalte ist, desto wichtiger ist dieses Wort zur Bestimmung des jeweiligen Merkmals.\n",
    "\n",
    "**Gewichtsmatrix:**  \n",
    "Jede Zeile beschreibt einen Artikel, jede Spalte die Gewichtung des jeweiligen Merkmals. Je höher der Wert, desto präsenter ist das jeweilige Merkmal im Artikel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Wie werden in Numpy zwei Arrays (Typ numpy.array) \n",
    "\t* im Sinne der Matrixmultiplikation miteinander multipliziert?\n",
    "\t* elementweise multipliziert?\n",
    "* Wie wird die Transponierte eines Numpy-Arrays berechnet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Mit dem Befehl 'numpy.dot' lassen sich zwei Matrizen im Sinne der Matrixmultiplikation miteinander multiplizieren.**  \n",
    "**Der Befehl 'numpy.multiply' multipliziert die beiden gegebenen Matrizen elementweise. Das selbe verhalten kann mit zwei numpy arrays mit 'a1 * a2' erreicht werden.**  \n",
    "\n",
    "**Die Transponierte eines Numpy-Array lässt sich mit dem Befehl 'numpy.matrix.transpose' berechnen.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Versuchsdurchführung\n",
    "Die in diesem Versuch einzubindenden Feeds sind in der unten stehenden Liste _feedlist_ definiert. Die aus dem vorigen Vesuch bereits bekannte Funktion _stripHTML()_ ist ebenfalls gegeben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import feedparser\n",
    "\n",
    "feedlist=['http://feeds.reuters.com/reuters/topNews',\n",
    "          'http://feeds.reuters.com/reuters/businessNews',\n",
    "          'http://feeds.reuters.com/reuters/worldNews',\n",
    "          'http://feeds2.feedburner.com/time/world',\n",
    "          'http://feeds2.feedburner.com/time/business',\n",
    "          'http://feeds2.feedburner.com/time/politics',\n",
    "          'http://rss.cnn.com/rss/edition.rss',\n",
    "          'http://rss.cnn.com/rss/edition_world.rss',\n",
    "          'http://www.nytimes.com/services/xml/rss/nyt/GlobalHome.xml',\n",
    "          'http://feeds.nytimes.com/nyt/rss/Business',\n",
    "          'http://www.nytimes.com/services/xml/rss/nyt/World.xml',\n",
    "          'http://www.nytimes.com/services/xml/rss/nyt/Economy.xml'\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stripHTML(h):\n",
    "  p=''\n",
    "  s=0\n",
    "  for c in h:\n",
    "    if c=='<': s=1\n",
    "    elif c=='>':\n",
    "      s=0\n",
    "      p+=' '\n",
    "    elif s==0: p+=c\n",
    "  return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anlegen der Artikel/Wort-Matrix\n",
    "\n",
    "### Die Funktion _getarticlewords()_\n",
    "Schreiben Sie eine Funktion _getarticlewords()_, die folgende Elemente zurückgibt:\n",
    "\n",
    "* _allwords:_ ist ein Dictionary dessen Keys die Worte aller gesammelten Artikel sind. Der zu jedem Key gehörende Wert ist die Anzahl, wie oft das Wort insgesamt vorkommt.\n",
    "* _articlewords:_ ist eine Liste mit so vielen Elementen wie Artikel in der Sammlung sind. Jedes Listenelement ist ein Dictionary, welches die Worte des jeweiligen Artikels als Key enthält und als Wert die Worthäufigkeit.\n",
    "* _articletitles_ ist eine Liste mit so vielen Elementen wie Artikel in der Sammlung sind. Jedes Element ist der Artikeltitel als String."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common_words = ['said', 'today', 'yesterday', 'frequently', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday',\n",
    "               'saturday', 'sunday', 'four']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getwordsTFStop(docText):    \n",
    "    sentence = nltk.regexp_tokenize(docText, r\"[\\wäöüÄÖÜß]+\")\n",
    "    \n",
    "    tokens = [x.lower() for x in sentence if x.lower() not in stops]\n",
    "    \n",
    "    wordDict = { x: tokens.count(x) for x in tokens }\n",
    "    \n",
    "    return wordDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für das Parsing der Feeds soll wieder das Modul _feedparser_ eingesetzt werden. Die zu einer Nachricht gehörenden Wörter sollen die Wörter des Elements _title_ und die Wörter des Elements _description_ sein (siehe voriger Versuch). Allerdings sollen hier nicht alle Wörter eingebunden werden, sondern wie im vorigen Versuch eine Methode _getwords()_ implementiert werden, welche nur die _relevanten_ Wörter zurückgibt. Die Frage welche Wörter relevant sind ist nicht eindeutig beantwortbar. Sie können sich hierzu eigene Antworten einfallen lassen. Auf jeden Fall sollten aber die Stopwörter ignoriert werden. Hierzu kann z.B. die Stopwortliste von NLTK angewandt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_article_words():\n",
    "    comb_text = ''\n",
    "    single_texts = []\n",
    "    article_titles = []\n",
    "    \n",
    "    for feed in feedlist:\n",
    "        f=feedparser.parse(feed)\n",
    "        for e in f.entries:\n",
    "            if e.get('description', 0):\n",
    "                article_titles.append(e.title)\n",
    "                single_texts.append(stripHTML(e.title+'\\n'+e.description))\n",
    "                comb_text += stripHTML(e.title+'\\n'+e.description + '\\n')\n",
    "                \n",
    "    all_words = getwordsTFStop(comb_text)\n",
    "    remove_list = get_remove_list(all_words, single_texts)\n",
    "    article_words = [({k: v for k, v in getwordsTFStop(text).items() if k not in remove_list}, index)\\\n",
    "                     for index, text in enumerate(single_texts)]\n",
    "    \n",
    "    #remove the title for the empty dicts\n",
    "    to_remove = []\n",
    "    for word_dict, index in article_words:\n",
    "        if len(word_dict) == 0:\n",
    "            to_remove.append(article_titles[index])\n",
    "            \n",
    "    article_titles = [article for article in article_titles if article not in to_remove]\n",
    "            \n",
    "    article_words = [w_dict for w_dict, index in article_words if len(w_dict) > 0]\n",
    "        \n",
    "    return (all_words, article_words, article_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem alle relevanten Wörter aller Nachrichten gesammelt sind, sollte eine weitere Bereinigung stattfinden, die \n",
    "\n",
    "* alle Wörter, die weniger als 4 mal vorkommen\n",
    "* alle Wörter, die in mehr als 30% aller Dokumente vorkommen\n",
    "\n",
    "entfernt. \n",
    "\n",
    "Durch dieses Herausfiltern nicht relevanter Wörter kann es vorkommen, dass einzelne Artikel keine relevanten Wörter mehr enthalten. Diese Artikel sollen dann ganz ignoriert werden. D.h. unter anderem, dass diese Artikel auch nicht in _articlewords_ und _articletitles_ erscheinen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_remove_list(all_words, single_texts):\n",
    "    MIN_OCCURENCE = 4\n",
    "    MAX_OCCURENCE = 0.3\n",
    "    \n",
    "    remove_list = []\n",
    "    \n",
    "    for key, value in all_words.items():\n",
    "        key_occurrence = 0\n",
    "        for text in single_texts:\n",
    "            if key in text:\n",
    "                key_occurrence += 1\n",
    "        \n",
    "        if value < 4 or key_occurrence > (len(single_texts) / 3):\n",
    "            remove_list.append(key)\n",
    "            del all_words[key]\n",
    "    \n",
    "    return remove_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_words, article_words, article_titles = get_article_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"Exclusive: Risking Beijing's ire, Vietnam begins dredging on South China Sea reef\",\n",
       " u'Trump expected to name CKE Restaurants CEO to head Labor Department: source',\n",
       " u'Trump to nominate Pruitt to lead U.S. environmental agency: statement',\n",
       " u'Quake magnitude 6.5 reported off coast of Northern California: USGS',\n",
       " u'U.S. jobless claims drop from five-month high',\n",
       " u'Poll: U.S. economic outlook unmoved so far after Trump win',\n",
       " u'ECB to scale back asset buys as it extends to end-2017',\n",
       " u\"Syrian army's Aleppo advance slows but victory in sight\",\n",
       " u'Iraqi troops retreat after Mosul hospital battle',\n",
       " u\"British spy chief says Islamic State plotting attacks as Russia makes 'desert' of Syria\"]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_titles[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die Funktion _makematrix()_\n",
    "Schreiben Sie eine Funktion _makematrix()_, die aus dem Dictionary _allwords_ und der Liste _articlewords_ (vorige Aufgabe) die Artikel-/Wort-Matrix generiert. Die Einträge in der Matrix sollen die Häufigkeiten der Wörter im jeweiligen Dokument sein (term frequency tf). Die Artikel-/Wort-Matrix soll als 2-dimensionales Numpy Array angelegt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_matrix(all_words, article_words):\n",
    "    return np.array([[article.get(word, 0) for word in all_words.keys()] for article in article_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = make_matrix(all_words, article_words)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Die Nicht Negative Matrix Faktorisierung\n",
    "Die Implementierung der NNMF ist entsprechend der Beschreibung im Theoriekapitel durchzuführen.\n",
    "\n",
    "* Implementieren Sie die Funktion _cost(A,B)_. Dieser Funktion werden zwei Numpy-Matrizen $A$ und $B$ übergeben. Zurück geliefert werden die nach oben angegebener Formel berechneten Kosten $k$. Diese Funktion wird von der im folgenden beschriebenen Funktion _nnmf(A,m,it)_ benutzt.\n",
    "* Implementieren Sie die Funktion __nnmf(A,m,it)__. In dieser Funktion soll der oben beschriebene Algorithmus für die Nicht-negative Matrix Faktorisierung ausgeführt werden. Der Funktion wird die zu faktorisierende Matrix $A$, die Anzahl der Merkmale $m$ und die Anzahl der Iterationen $it$ übergeben. Die Funktion gibt die gefundenen Faktoren $W$ und $H$ zurück. In jeder Iteration sollen mit der Funktion __cost(A,B)__ die Kosten berechnet werden. Sobald die Kostenabnahme pro 10 Iterationen kleiner als $2$ ist oder eine maximale Anzahl von Iterationen ($maxIt=200$) erreicht ist, soll der Algorithmus mit der Rückgabe der Faktoren $W$ und $H$ terminieren.     \n",
    "\n",
    "\n",
    "Tipp für die Implementierung elementweiser Operationen von Matrizen: Für elementweise Operationen müssen in Python/Numpy nicht alle Elemente über Schleifen explizit berechnet werden. Eine elementweise Anpassung aller Matrixelemente kann kompakt programmiert werden indem die beteiligten Matrizen für diese Operationen als Arrays implementiert werden. Sollen z.B. die beiden gleich großen Numpy Arrays $U$ und $V$ elementweise multipliziert werden, dann wäre der entsprechende Programmcode einfach _U*V_.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "matrix_A = np.random.rand(5, 5)\n",
    "\n",
    "def nnmf_cost (A, B):\n",
    "    return np.sum(np.square((A - B)))\n",
    "\n",
    "\n",
    "def nnmf (A, m, maxIt):        \n",
    "    matrix_W = np.random.rand(len(A), m)\n",
    "    matrix_H = np.random.rand(m, len(A[0]))\n",
    "\n",
    "    temp_A = matrix_W.dot(matrix_H)\n",
    "    prev_cost = cost = nnmf_cost(A, temp_A)    \n",
    "    \n",
    "    for i in range(maxIt):        \n",
    "        #print cost\n",
    "        \n",
    "        # terminate if last 10 iterations reduced costs by less than 2\n",
    "        if i != 0 and i % 10 == 0:\n",
    "            if (prev_cost - cost) < 2:\n",
    "                return ( matrix_W, matrix_H )\n",
    "            \n",
    "            prev_cost = cost\n",
    "        \n",
    "        # recalculate H\n",
    "        matrix_numerator = np.transpose(matrix_W).dot(A)\n",
    "        matrix_denominator = np.transpose(matrix_W).dot(matrix_W).dot(matrix_H)\n",
    "    \n",
    "        for row in range(len(matrix_H)):\n",
    "            for col in range(len(matrix_H[0])):                \n",
    "                matrix_H[row][col] = matrix_H[row][col] * (matrix_numerator[row][col] / matrix_denominator[row][col])\n",
    "        \n",
    "        # recalculate W        \n",
    "        matrix_numerator = A.dot(np.transpose(matrix_H))\n",
    "        matrix_denominator = matrix_W.dot(matrix_H).dot(np.transpose(matrix_H))\n",
    "        \n",
    "        for row in range(len(matrix_W)):\n",
    "            for col in range(len(matrix_W[0])):                \n",
    "                matrix_W[row][col] = matrix_W[row][col] * (matrix_numerator[row][col] / matrix_denominator[row][col])\n",
    "    \n",
    "        ## recalculate cost\n",
    "        temp_A = matrix_W.dot(matrix_H)\n",
    "        cost = nnmf_cost(A, temp_A)    \n",
    "    \n",
    "    \n",
    "    return ( matrix_W, matrix_H )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matrix_W, matrix_H = nnmf(matrix, 10, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anzeige der Merkmale und der Gewichte\n",
    "\n",
    "Im vorigen Abschnitt wurde die Merkmalsmatrix $H$ und die Gewichtsmatrix $W$ berechnet. Diese Matrizen können natürlich am Bildschirm ausgegeben werden, was jedoch nicht besonders informativ ist. Aus den Matrizen können jedoch die Antworten für die folgenden interessanten Fragen berechnet werden:\n",
    "\n",
    "* In welchen Artikeln sind welche Merkmale stark vertreten?\n",
    "* Wie lassen sich die insgesamt $m$ Merkmale beschreiben, so dass aus dieser Merkmalsbeschreibung klar wird, welches Thema den Artikeln, in denen das Merkmal stark vertreten ist, behandelt wird? \n",
    " \n",
    "Die Antwort auf die erste Frage ergibt sich aus der Gewichtsmatrix $W$. Für die Beantwortung der zweiten Frage wird die Merkmalsmatrix $H$ herangezogen.\n",
    "\n",
    "### Beschreibung der Merkmale\n",
    "\n",
    "Die Merkmalsmatrix $H$ beschreibt, wie stark die Worte aus _wordvec_ in jedem Merkmal enthalten sind. Jede Zeile von $H$ gehört zu einem Merkmal, jede Spalte von $H$ gehört zu einem Wort in _wordvec_.\n",
    "\n",
    "Es bietet sich an jedes Merkmal einfach durch die $N=6$ Wörter aus _wordvec_ zu beschreiben, welche am stärksten in diesem Merkmal enthalten sind. Hierzu muss für jedes Merkmal die entsprechende Zeile in $H$ nach den $N=6$ größten Werten durchsucht bzw. geordnet werden. Die entsprechenden Spalten dieser Matrixelemente verweisen dann auf die $N=6$ wichtigsten Worte des Merkmals.\n",
    "\n",
    "Tipp für die Implementierung: Legen Sie für jedes Merkmal $i$ eine Liste an. Die Listenlänge ist durch die Anzahl der Worte in _wordvec_ (d.h. die Anzahl der Spalten in $H$) gegeben. Jedes Listenelement $j$ enthält selbst wieder 2 Elemente: An erster Stelle den entsprechenden Wert $H_{i,j}$ der Merkmalsmatrix, an der zweiten Stelle das $j.$-te Wort in _wordvec_. Nachdem die Liste angelegt ist, kann sie mit _listname.sort()_ in aufsteigender Reihenfolge sortiert werden. Die abnehmende Sortierung erhält man mit _listname.sort().reverse()_. Danach geben die $N=6$ ersten Listenelemente die für das Merkmal $i$ wichtigsten Worte an.\n",
    "\n",
    "   \n",
    "### Präsenz der Merkmale in den Artikeln\n",
    "\n",
    "Die Gewichtsmatrix $W$ beschreibt, wie stark die $m$ Merkmale in den Artikeln aus _articletitles_ enthalten sind. Jede Zeile von $W$ gehört zu einem Artikel, jede Spalte von $W$ gehört zu einem Merkmal.\n",
    "Die Berechnung der $M=2$ gewichtigsten Merkmale für jeden Artikel in _articletitles_ kann analog zu der oben beschriebenen Berechnung der $N=6$ wichtigsten Worte eines Merkmals berechnet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "matrix_A = np.random.rand(5, 5)\n",
    "\n",
    "def nnmf_cost (A, B):\n",
    "    return np.sum(np.square((A - B)))\n",
    "\n",
    "\n",
    "def nnmf (A, m, maxIt):        \n",
    "    matrix_W = np.random.rand(len(A), m)\n",
    "    matrix_H = np.random.rand(m, len(A[0]))\n",
    "\n",
    "    temp_A = matrix_W.dot(matrix_H)\n",
    "    prev_cost = cost = nnmf_cost(A, temp_A)    \n",
    "    \n",
    "    for i in range(maxIt):      \n",
    "        # terminate if last 10 iterations reduced costs by less than \n",
    "        if i != 0 and i % 10 == 0:\n",
    "            if (prev_cost - cost) < 2:\n",
    "                return ( matrix_W, matrix_H, cost )\n",
    "            \n",
    "            prev_cost = cost\n",
    "            \n",
    "        # recalculate H\n",
    "        matrix_numerator = np.transpose(matrix_W).dot(A)\n",
    "        matrix_denominator = np.transpose(matrix_W).dot(matrix_W).dot(matrix_H)\n",
    "    \n",
    "        for row in range(len(matrix_H)):\n",
    "            for col in range(len(matrix_H[0])):                \n",
    "                matrix_H[row][col] = matrix_H[row][col] * (matrix_numerator[row][col] / matrix_denominator[row][col])\n",
    "        \n",
    "        # recalculate W        \n",
    "        matrix_numerator = A.dot(np.transpose(matrix_H))\n",
    "        matrix_denominator = matrix_W.dot(matrix_H).dot(np.transpose(matrix_H))\n",
    "        \n",
    "        for row in range(len(matrix_W)):\n",
    "            for col in range(len(matrix_W[0])):                \n",
    "                matrix_W[row][col] = matrix_W[row][col] * (matrix_numerator[row][col] / matrix_denominator[row][col])\n",
    "    \n",
    "        ## recalculate cost\n",
    "        temp_A = matrix_W.dot(matrix_H)\n",
    "        cost = nnmf_cost(A, temp_A)    \n",
    "    \n",
    "    \n",
    "    return ( matrix_W, matrix_H, cost )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1278.23130639\n",
      "[u'founder', u'iraq', u'pick', u'africa', u'two', u'pearl']\n",
      "[u'founder', u'small', u'cienfuegos', u'pick', u'africa', u'two']\n",
      "[u'founder', u'york', u'poll', u'turkey', u'mars', u'wants']\n",
      "[u'aceh', u'cienfuegos', u'africa', u'two', u'pearl', u'6']\n",
      "[u'founder', u'cienfuegos', u'iraq', u'pick', u'africa', u'two']\n",
      "[u'founder', u'middle', u'buys', u'trains', u'income', u'michael']\n",
      "[u'founder', u'4', u'income', u'michael', u'bookstore', u'sales']\n",
      "[u'aceh', u'8221', u'girl', u'oklahoma', u'cars', u'retired']\n",
      "[u'founder', u'need', u'wall', u'4', u'income', u'july']\n",
      "[u'aceh', u'oakland', u'airline', u'bookstore', u'buying', u'sales']\n",
      "[u'founder', u'buying', u'5', u'month', u'hye', u'cienfuegos']\n",
      "[u'aceh', u'nine', u'states', u'plant', u'seoul', u'plane']\n",
      "[u'aceh', u'trains', u'need', u'4', u'income', u'michael']\n",
      "[u'founder', u'iraq', u'two', u'pearl', u'6', u'markets']\n",
      "[u'founder', u'bookstore', u'buying', u'sales', u'britain', u'5']\n",
      "[u'founder', u'oakland', u'airline', u'bookstore', u'sales', u'5']\n",
      "[u'founder', u'markets', u'town', u'nine', u'states', u'plane']\n",
      "[u'founder', u'name', u'million', u'california', u'advance', u'british']\n",
      "[u'stay', u'wants', u'minister', u'former', u'case', u'middle']\n",
      "[u'founder', u'retired', u'2', u'using', u'organization', u'say']\n",
      "[u'aceh', u'hye', u'cienfuegos', u'africa', u'two', u'pearl']\n",
      "[u'mosul', u'mae', u'man', u'still', u'house', u'policy']\n",
      "[u'founder', u'airline', u'buying', u'sales', u'general', u'britain']\n",
      "[u'founder', u'deep', u'syrian', u'5', u'small', u'cienfuegos']\n",
      "[u'founder', u'follow', u'hye', u'cienfuegos', u'two', u'markets']\n",
      "[u'founder', u'income', u'michael', u'central', u'industry', u'bookstore']\n",
      "[u'aceh', u'saving', u'first', u'residents', u'california', u'cars']\n",
      "[u'founder', u'deep', u'syrian', u'5', u'hye', u'small']\n",
      "[u'founder', u'around', u'michael', u'tower', u'legal', u'industry']\n",
      "[u'aceh', u'saving', u'say', u'dredging', u'residents', u'great']\n",
      "[u'aceh', u'killed', u'name', u'renzi', u'cars', u'california']\n",
      "[u'aceh', u'man', u'mae', u'korean', u'house', u'mosul']\n",
      "[u'big', u'american', u'oklahoma', u'cars', u'california', u'advance']\n",
      "[u'founder', u'found', u'house', u'portland', u'mae', u'islamic']\n",
      "[u'aceh', u'legal', u'central', u'industry', u'airline', u'bookstore']\n"
     ]
    }
   ],
   "source": [
    "(matrix_weights, matrix_topics, cost) = nnmf(matrix, 35, 200)\n",
    "\n",
    "\n",
    "print cost\n",
    "\n",
    "for topic in matrix_topics:\n",
    "    words = all_words.keys()\n",
    "    indices = np.argsort(topic)\n",
    "    print [words[i] for i in indices[:6]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementierung\n",
    "\n",
    "Implementieren Sie eine Funktion _showfeatures(w,h,titles,wordvec)_, welche wie oben beschrieben für jeden Artikel die $M=2$ wichtigsten Merkmale am Bildschirm ausgibt. Dabei soll jedes Merkmal durch die 6 wichtigsten Wörter dieses Merkmals angegeben werden. Siehe Beispielausgabe unten.  \n",
    "\n",
    "Übergabeparameter der Funktion sind die Merkmalsmatrix $H$, die Gewichtungsmatrix $W$, die Liste aller Artikeltitel _articletitles_ und die Liste aller Worte _wordvec_.\n",
    "\n",
    "\n",
    "Beispiel fuer Ausgabe:\n",
    "\n",
    "[(13.54131155883748, 13, u'Putin vows payback after confirmation of Egypt plane bomb'),\n",
    "\n",
    "(2.2466669548146254, 9, u'Putin vows payback after confirmation of Egypt plane bomb')]\n",
    "\n",
    "----- ['plane', 'egypt', 'russia', 'month', 'killing', 'putin']\n",
    "\n",
    "----- ['airport', 'russian', 'crash', 'egypt', 'security', 'officials']\n",
    "\n",
    "Die Ausgabe ist wie folgt zu interpretieren:\n",
    "* Für den Artikel _Putin vows payback after confirmation of Egypt plane bomb_ ist \n",
    "    * das wichtigste Merkmal durch die 6 Wörter _plane_, _egypt_, _russia_, _month_, _killing_, _putin_ definiert. Das Gewicht dieses Merkmals im Artikel ist 13.54\n",
    "    * das zweitwichtigste Merkmal durch die 6 Wörter _airport_, _russian_, _crash_, _egypt_, _security_, _officials_ definiert. Das Gewicht dieses Merkmals im Artikel ist 2.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_for_feature(feature):\n",
    "    words = all_words.keys()\n",
    "    indices = (-np.array(feature)).argsort()[:6]\n",
    "    return [words[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_features(w,h,titles,wordvec):\n",
    "    for index, article in enumerate(article_titles[:25]): #only print 25 article-titles\n",
    "        curr_w = w[index]\n",
    "        \n",
    "        w_indices = (-np.array(curr_w)).argsort()[:2]\n",
    "        \n",
    "        print article\n",
    "        for i in w_indices:\n",
    "            print curr_w[i], get_for_feature(h[i])\n",
    "            \n",
    "        print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exclusive: Risking Beijing's ire, Vietnam begins dredging on South China Sea reef\n",
      "5.83640347983 [u'south', u'president', u'korea', u'park', u'hye', u'geun']\n",
      "3.25027867877 [u'state', u'islamic', u'chief', u'reuters', u'attacks', u'syria']\n",
      "\n",
      "Trump expected to name CKE Restaurants CEO to head Labor Department: source\n",
      "5.76238053155 [u'trump', u'donald', u'president', u'elect', u'j', u'win']\n",
      "2.26820994406 [u'last', u'month', u'week', u'court', u'five', u'high']\n",
      "\n",
      "Trump to nominate Pruitt to lead U.S. environmental agency: statement\n",
      "14.7746612166 [u'pruitt', u'trump', u'scott', u'lead', u'environmental', u'general']\n",
      "1.33355291073 [u'reuters', u'bank', u'central', u'thursday', u'2017', u'would']\n",
      "\n",
      "Quake magnitude 6.5 reported off coast of Northern California: USGS\n",
      "2.83313627527 [u'reuters', u'bank', u'central', u'thursday', u'2017', u'would']\n",
      "1.99240033853 [u'state', u'islamic', u'chief', u'reuters', u'attacks', u'syria']\n",
      "\n",
      "U.S. jobless claims drop from five-month high\n",
      "9.22943931692 [u'last', u'month', u'week', u'court', u'five', u'high']\n",
      "0.688464779478 [u'reuters', u'bank', u'central', u'thursday', u'2017', u'would']\n",
      "\n",
      "Poll: U.S. economic outlook unmoved so far after Trump win\n",
      "6.51934980033 [u'trump', u'donald', u'president', u'elect', u'j', u'win']\n",
      "4.09901505977 [u'reuters', u'bank', u'central', u'thursday', u'2017', u'would']\n",
      "\n",
      "ECB to scale back asset buys as it extends to end-2017\n",
      "11.4509284322 [u'reuters', u'bank', u'central', u'thursday', u'2017', u'would']\n",
      "0.534124155264 [u'8217', u'8221', u'trump', u'election', u'big', u'donald']\n",
      "\n",
      "Syrian army's Aleppo advance slows but victory in sight\n",
      "16.846489957 [u'aleppo', u'syrian', u'city', u'army', u'syria', u'sight']\n",
      "0.722041115926 [u'reuters', u'bank', u'central', u'thursday', u'2017', u'would']\n",
      "\n",
      "Iraqi troops retreat after Mosul hospital battle\n",
      "8.39217748143 [u'state', u'islamic', u'chief', u'reuters', u'attacks', u'syria']\n",
      "0.321738445341 [u'8217', u'8221', u'trump', u'election', u'big', u'donald']\n",
      "\n",
      "British spy chief says Islamic State plotting attacks as Russia makes 'desert' of Syria\n",
      "15.9506898991 [u'state', u'islamic', u'chief', u'reuters', u'attacks', u'syria']\n",
      "0.248313812158 [u'aleppo', u'syrian', u'city', u'army', u'syria', u'sight']\n",
      "\n",
      "Wall Street hits new high as post-election rally roars ahead\n",
      "3.02427148218 [u'new', u'today', u'york', u'city', u'bookstore', u'mars']\n",
      "2.23707610733 [u'8217', u'8221', u'trump', u'election', u'big', u'donald']\n",
      "\n",
      "U.S. jobless claims drop from five-month high\n",
      "9.2294383313 [u'last', u'month', u'week', u'court', u'five', u'high']\n",
      "0.688460855031 [u'reuters', u'bank', u'central', u'thursday', u'2017', u'would']\n",
      "\n",
      "Exclusive: Bangladesh panel finds insiders negligent in central bank heist\n",
      "9.85994680518 [u'reuters', u'bank', u'central', u'thursday', u'2017', u'would']\n",
      "0.484121000133 [u'state', u'islamic', u'chief', u'reuters', u'attacks', u'syria']\n",
      "\n",
      "ECB to scale back asset buys as it extends to end-2017\n",
      "11.4513845656 [u'reuters', u'bank', u'central', u'thursday', u'2017', u'would']\n",
      "0.533523320677 [u'8217', u'8221', u'trump', u'election', u'big', u'donald']\n",
      "\n",
      "Deutsche Bank's 'smoking gun' evidence to expand U.S. silver rigging case\n",
      "6.77241438609 [u'new', u'today', u'york', u'city', u'bookstore', u'mars']\n",
      "2.67298645624 [u'reuters', u'bank', u'central', u'thursday', u'2017', u'would']\n",
      "\n",
      "China's Fujian drops Aixtron bid after Obama blocks deal\n",
      "1.73789726579 [u'mattis', u'obama', u'gen', u'president', u'war', u'era']\n",
      "1.63620328819 [u'state', u'islamic', u'chief', u'reuters', u'attacks', u'syria']\n",
      "\n",
      "IMF says retains confidence in Lagarde over French trial\n",
      "2.14649397671 [u'reuters', u'bank', u'central', u'thursday', u'2017', u'would']\n",
      "1.95993899688 [u'state', u'islamic', u'chief', u'reuters', u'attacks', u'syria']\n",
      "\n",
      "WardsAuto sees 2017 U.S. auto sales 17.2 million, 17.4 million in 2016\n",
      "9.6952673263 [u'reuters', u'bank', u'central', u'thursday', u'2017', u'would']\n",
      "5.6294178933 [u'new', u'today', u'york', u'city', u'bookstore', u'mars']\n",
      "\n",
      "SEC chair says mistake to weaken post-crisis financial reforms\n",
      "3.30845179568 [u'reuters', u'bank', u'central', u'thursday', u'2017', u'would']\n",
      "1.80911114317 [u'state', u'islamic', u'chief', u'reuters', u'attacks', u'syria']\n",
      "\n",
      "Poll: U.S. economic outlook unmoved so far after Trump win\n",
      "6.52169695658 [u'trump', u'donald', u'president', u'elect', u'j', u'win']\n",
      "4.10032375098 [u'reuters', u'bank', u'central', u'thursday', u'2017', u'would']\n",
      "\n",
      "Syrian army's Aleppo advance slows but victory in sight\n",
      "16.8468499825 [u'aleppo', u'syrian', u'city', u'army', u'syria', u'sight']\n",
      "0.722432506711 [u'reuters', u'bank', u'central', u'thursday', u'2017', u'would']\n",
      "\n",
      "Exclusive: Risking Beijing's ire, Vietnam begins dredging on South China Sea reef\n",
      "5.83639991447 [u'south', u'president', u'korea', u'park', u'hye', u'geun']\n",
      "3.25025470943 [u'state', u'islamic', u'chief', u'reuters', u'attacks', u'syria']\n",
      "\n",
      "Iraqi troops retreat after Mosul hospital battle\n",
      "8.39125457983 [u'state', u'islamic', u'chief', u'reuters', u'attacks', u'syria']\n",
      "0.319014408529 [u'8217', u'8221', u'trump', u'election', u'big', u'donald']\n",
      "\n",
      "South Korea parliament introduces bill to impeach Park; vote due Friday\n",
      "8.90989314403 [u'south', u'president', u'korea', u'park', u'hye', u'geun']\n",
      "1.23602840904 [u'reuters', u'bank', u'central', u'thursday', u'2017', u'would']\n",
      "\n",
      "Italy president starts talks to seek way out of political crisis\n",
      "2.04352626242 [u'south', u'president', u'korea', u'park', u'hye', u'geun']\n",
      "1.77468577421 [u'reuters', u'bank', u'central', u'thursday', u'2017', u'would']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_features(matrix_W, matrix_H, article_titles, all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgaben\n",
    "\n",
    "1.Analysieren Sie die berechneten Topics indem Sie sich überlegen ob die gefundenen 6 Wörter pro Topic wirklich Themen beschreiben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bad:**  \n",
    "    - [u'aceh', u'retired', u'11', u'000', u'dredging', u'america']\n",
    "    - [u'increase', u'turkey', u'mars', u'buys', u'trains', u'around']\n",
    "\n",
    "**Bei den hier aufgeführten schlechten Topics erkennt man leider sehr wenig bis gar keinen Zusammenhang zwischen den Merkmalen. Z.b. wird 'turkey' mit 'mars' und 'trains' zusammengefasst, was leider gar keinen Sinn ergibt.**\n",
    "\n",
    "**Good**\n",
    "\n",
    "\n",
    "    - [u'pruitt', u'trump', u'scott', u'lead', u'environmental', u'general']\n",
    "    - [u'state', u'islamic', u'chief', u'reuters', u'attacks', u'syria']\n",
    "\n",
    "**Bei diesen Beispielen sind zwar auch nicht alle Begriffe thematisch zusammenhängend aber es wird klar, dass es sich bei dem ersten Beispiel um einen ein Thema mit Trum und der Klimaveränderung handelt ('Trump Picks Scott Pruitt, Climate Change Denialist, to Lead E.P.A.').**\n",
    "\n",
    "**Beim 2. Beispiel um ein Thema in dem der Islamische Staat eine große Rolle spielt. ('British spy chief says Islamic State plotting attacks as Russia makes 'desert' of Syria')**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Verändern Sie die Parameter der NNMF (Anzahl der Topics $m$, Anzahl der Iterationen). Bei welcher Einstellung der Parameter erhalten Sie das für sie sinnvollste Resultat (sinnvolle Topics)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Man erkennt, dass die Kosten sich relativ schnell einer Asymptote annähert und man expotentiel mehr Iterationen benötigt um nur noch marginale Verbesserungen zu bekommen. 100 bis 200 Iterationen sind ausreichend, danach wird die topic extraction nicht mehr bemerkbar besser. Ein viel größerer Einfluss hat welche W-, H-Matritzen zu Beginn zufällig generiert werden.** \n",
    "\n",
    "```\n",
    "Iterationen - Kosten\n",
    "\n",
    "10            1273.84807468\n",
    "100           1053.96711681\n",
    "200           1043.08143901\n",
    "400           1036.33627363\n",
    "1000          1034.91014625\n",
    "2000          1034.53171712\n",
    "```\n",
    "\n",
    "**Die Bewertung der Qualität der topic extraction ist natürlich auch vom persönlichen Empfinden abhängig. Wir haben    bei 30 bis 40 die besten Ergebnisse erhalten. Bei höheren Werten wie z.B. 100 nimmt die Kostenfunktion zwar ab, aber es findet auch keine relevante Gruppierung der Artikel mehr statt.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Wie kann die _getwords()_ Methode verbessert werden, so dass noch bedeutsamere Topics gefunden werden? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Eine mögliche Optimierung wäre, dass man zusätzlich zum Entfernen der Stopwords auch noch weitere Wörter vorab Filtert.**  \n",
    "\n",
    "**Zu diesen Worten gerhören z.B. \"said\"(\"sagte\"), \"today\"(\"heute\") usw. -> Also Wörter welche in Artikeln und News extrem häufig vorkommen.**  \n",
    "**Außerdem sind Wochentage und allgemein Zeitangaben weniger relevant und können deshalb auch herausgefiltert werden.**  \n",
    "\n",
    "**Natürlich sind die zusätzlich filterbaren Wörter speziell auf die gesuchten Topics anzupassen.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
